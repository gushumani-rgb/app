name: Build index JSON & service worker

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  POSTS_DIR: posts
  INDEX_PATH: index.json

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 pyyaml python-dateutil pytz

      - name: Build index.json
        run: |
          python - <<'PY'
          import os, json, re
          from pathlib import Path
          from bs4 import BeautifulSoup
          import yaml
          from dateutil import parser as dateparser
          from datetime import datetime, timezone, timedelta

          TZ_OFFSET = timezone(timedelta(hours=2))  # UTC+2
          root = Path.cwd()
          posts_root = root / os.environ.get("POSTS_DIR", "posts")
          index_path = root / os.environ.get("INDEX_PATH", "index.json")

          def read_file(p):
              try:
                  return p.read_text(encoding='utf-8', errors='ignore')
              except Exception:
                  return ""

          def parse_front_matter(text):
              if text.startswith('---'):
                  parts = text.split('---', 2)
                  if len(parts) >= 3:
                      try:
                          fm = yaml.safe_load(parts[1]) or {}
                          body = parts[2]
                          return fm, body
                      except Exception:
                          return {}, text
              return {}, text

          def extract_youtube_id(url):
              if not url:
                  return None
              try:
                  if "youtu.be/" in url:
                      return url.split("youtu.be/")[1].split("?")[0]
                  elif "youtube.com/watch" in url:
                      params = dict(x.split("=") for x in url.split("?")[1].split("&") if "=" in x)
                      return params.get("v")
              except Exception:
                  return None
              return None

          def extract_youtube_thumbnail(text, link=None):
              if link:
                  vid = extract_youtube_id(link)
                  if vid:
                      return f"https://img.youtube.com/vi/{vid}/hqdefault.jpg"
              urls = re.findall(r'(https?://(?:www\.)?(?:youtube\.com/watch\?v=|youtu\.be/)([A-Za-z0-9_-]{11}))', text)
              if urls:
                  video_id = urls[0][1]
                  return f"https://img.youtube.com/vi/{video_id}/hqdefault.jpg"
              return None

          posts = []

          if posts_root.exists():
              for p in sorted(posts_root.rglob('*')):
                  if p.is_file() and p.suffix.lower() in ('.md', '.markdown', '.html', '.htm'):
                      text = read_file(p)
                      meta = {}
                      body = text

                      # File modified time fallback
                      try:
                          mtime_ts = p.stat().st_mtime
                          mtime_dt = datetime.fromtimestamp(mtime_ts, tz=TZ_OFFSET)
                      except Exception:
                          mtime_dt = datetime.now(TZ_OFFSET)

                      # Parse front matter
                      if p.suffix.lower() in ('.md', '.markdown'):
                          fm, body = parse_front_matter(text)
                          if fm:
                              meta.update(fm or {})
                      else:
                          soup = BeautifulSoup(text, 'html.parser')
                          if soup.title and soup.title.string:
                              meta['title'] = soup.title.string.strip()
                          desc = soup.find('meta', attrs={'name':'description'}) or soup.find('meta', attrs={'property':'og:description'})
                          if desc and desc.get('content'):
                              meta['excerpt'] = desc['content'].strip()
                          kw = soup.find('meta', attrs={'name':'keywords'})
                          if kw and kw.get('content'):
                              meta.setdefault('tags', [t.strip() for t in kw['content'].split(',') if t.strip()])

                      if not meta.get('title'):
                          m = re.search(r'^\s*#\s+(.+)$', body, re.MULTILINE)
                          meta['title'] = m.group(1).strip() if m else p.stem

                      if not meta.get('excerpt'):
                          soup2 = BeautifulSoup(body, 'html.parser')
                          ptag = soup2.find('p')
                          meta['excerpt'] = ptag.get_text(strip=True)[:300] if ptag else ''

                      # Featured / Image
                      meta['featuredImage'] = None
                      meta['image'] = None
                      yt_thumb = extract_youtube_thumbnail(body, meta.get('link'))
                      if yt_thumb:
                          meta['featuredImage'] = yt_thumb
                          meta['image'] = yt_thumb
                      else:
                          soup2 = BeautifulSoup(body, 'html.parser')
                          img_tag = soup2.find('img')
                          if img_tag and img_tag.get('src'):
                              meta['featuredImage'] = img_tag['src'].strip()
                              meta['image'] = meta['featuredImage']
                          elif 'featuredImage' in meta and meta['featuredImage']:
                              meta['featuredImage'] = meta['featuredImage'].strip()
                              meta['image'] = meta['featuredImage']

                      tags = meta.get('tags') or meta.get('tag') or meta.get('keywords') or []
                      if isinstance(tags, str):
                          tags = [t.strip() for t in re.split(r'[,\\s]+', tags) if t.strip()]
                      categories = meta.get('categories') or meta.get('category') or []
                      if isinstance(categories, str):
                          categories = [t.strip() for t in re.split(r'[,\\s]+', categories) if t.strip()]

                      text_only = re.sub(r'<[^>]+>', '', body)
                      text_only = re.sub(r'[`*_>#~\-]{1,}', ' ', text_only)
                      words = len(re.findall(r'\w+', text_only))
                      reading_time = max(1, int(round(words / 200))) if words > 0 else 1

                      rel = p.relative_to(root).as_posix()
                      try:
                          parent_rel = p.parent.relative_to(root).as_posix()
                      except Exception:
                          parent_rel = ""
                      if p.name.lower() in ('index.html', 'index.htm'):
                          slug = '/' + parent_rel.rstrip('/') + '/'
                          if slug == '//':
                              slug = '/'
                      else:
                          slug = '/' + rel

                      dt_obj = None
                      if 'date' in meta and meta['date']:
                          try:
                              parsed = dateparser.parse(str(meta['date']))
                              if parsed:
                                  if not parsed.tzinfo:
                                      parsed = parsed.replace(tzinfo=timezone.utc)
                                  dt_obj = parsed.astimezone(TZ_OFFSET)
                          except Exception:
                              dt_obj = None

                      if not dt_obj:
                          soup_date = BeautifulSoup(body, 'html.parser')
                          pdate = soup_date.find('p', class_='post-date')
                          if pdate:
                              try:
                                  parsed = dateparser.parse(pdate.get_text(strip=True))
                                  if parsed:
                                      if not parsed.tzinfo:
                                          parsed = parsed.replace(tzinfo=timezone.utc)
                                      dt_obj = parsed.astimezone(TZ_OFFSET)
                              except Exception:
                                  dt_obj = None

                      if not dt_obj:
                          dt_obj = mtime_dt

                      posts.append({
                          'title': meta.get('title'),
                          'date': dt_obj.isoformat(),
                          'excerpt': meta.get('excerpt', ''),
                          'path': slug,
                          'source': rel,
                          'tags': tags,
                          'categories': categories,
                          'reading_time': reading_time,
                          'word_count': words,
                          'featuredImage': meta.get('featuredImage'),
                          'image': meta.get('image')
                      })

          if not posts:
              posts = [{"title": "No posts yet", "excerpt": "", "path": "/", "source": ""}]

          def sort_key(item):
              try:
                  return dateparser.parse(item.get('date')).timestamp()
              except:
                  return 0

          posts_sorted = sorted(posts, key=sort_key, reverse=True)
          index_path.write_text(json.dumps(posts_sorted, indent=2, ensure_ascii=False), encoding='utf-8')
          print(f"Written {index_path} with {len(posts_sorted)} posts (UTC+2).")
          PY

      - name: Build service worker (sw.js) with prioritized recent images
        run: |
          python - <<'PY'
          import json, os
          from pathlib import Path
          from dateutil import parser as dateparser

          root = Path.cwd()
          index_path = root / os.environ.get("INDEX_PATH", "index.json")
          sw_path = root / "sw.js"

          PRIORITY_RECENT = 5
          PRECACHE = set()
          PRECACHE.update(["/", "/index.json"])

          try:
              data = json.loads(index_path.read_text(encoding="utf-8"))
          except Exception:
              data = []

          def parse_date(item):
              try:
                  return dateparser.parse(item.get('date'))
              except:
                  return None

          posts_sorted = sorted(data, key=parse_date, reverse=True)

          for post in posts_sorted[:PRIORITY_RECENT]:
              p = post.get("path")
              if p:
                  if not p.startswith("/"):
                      p = "/" + p
                  PRECACHE.add(p)
              for img in [post.get("featuredImage"), post.get("image")]:
                  if img:
                      PRECACHE.add(img.strip())

          for post in posts_sorted[PRIORITY_RECENT:]:
              p = post.get("path")
              if p:
                  if not p.startswith("/"):
                      p = "/" + p
                  PRECACHE.add(p)
              for img in [post.get("featuredImage"), post.get("image")]:
                  if img:
                      PRECACHE.add(img.strip())

          precache_list = sorted(list(PRECACHE))

          sw_content = f"""
          const CACHE_NAME = 'site-cache-v1';
          const PRECACHE = {json.dumps(precache_list, indent=2)};

          self.addEventListener('install', event => {{
            self.skipWaiting();
            event.waitUntil(
              caches.open(CACHE_NAME).then(cache => {{
                return Promise.all(
                  PRECACHE.map(u => cache.add(new Request(u, {{credentials: 'same-origin'}}))).catch(err => {{
                    console.warn('Some resources failed to cache:', err);
                  }})
                );
              }})
            );
          }});

          self.addEventListener('activate', event => {{
            event.waitUntil(
              caches.keys().then(keys => Promise.all(
                keys.filter(k => k !== CACHE_NAME).map(k => caches.delete(k))
              )).then(() => self.clients.claim())
            );
          }});

          self.addEventListener('fetch', event => {{
            const req = event.request;
            if (req.method !== 'GET') return;
            if (req.mode === 'navigate') {{
              event.respondWith(
                fetch(req).then(res => {{
                  const copy = res.clone();
                  caches.open(CACHE_NAME).then(cache => cache.put(req, copy));
                  return res;
                }}).catch(() => caches.match('/'))
              );
              return;
            }}
            event.respondWith(
              caches.match(req).then(cached => cached || fetch(req).then(networkRes => {{
                try {{
                  const url = new URL(req.url);
                  if (url.origin === location.origin || url.href.startsWith('http')) {{
                    const copy = networkRes.clone();
                    caches.open(CACHE_NAME).then(cache => cache.put(req, copy));
                  }}
                }} catch(e) {{}}
                return networkRes;
              }})).catch(() => caches.match('/'))
            );
          }});
          """
          sw_path.write_text(sw_content, encoding="utf-8")
          print(f"Written {sw_path} with {len(precache_list)} entries, recent {PRIORITY_RECENT} prioritized.")
          PY

      - name: Commit and push JSON + sw.js
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"

          # Ensure local branch matches remote
          git fetch origin main
          git reset --hard origin/main

          git add index.json sw.js

          if ! git diff --cached --quiet; then
            git commit -m "Update index.json and sw.js [skip ci]"
            git push origin main
          else
            echo "No changes to commit."
