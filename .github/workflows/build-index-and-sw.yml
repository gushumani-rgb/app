name: Build index & subscriptions JSON

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  POSTS_DIR: posts
  INDEX_PATH: index.json
  SUBS_PATH: subscriptions.json  # New

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 pyyaml python-dateutil pytz

      - name: Build index.json and subscriptions.json
        run: |
          python - <<'PY'
          import os, json, re
          from pathlib import Path
          from bs4 import BeautifulSoup
          import yaml
          from dateutil import parser as dateparser
          from datetime import datetime, timezone, timedelta

          # --- Config ---
          TZ_OFFSET = timezone(timedelta(hours=2))  # UTC+2
          root = Path.cwd()
          posts_root = root / os.environ.get("POSTS_DIR", "posts")
          index_path = root / os.environ.get("INDEX_PATH", "index.json")
          subs_path = root / os.environ.get("SUBS_PATH", "subscriptions.json")

          # --- Helper functions ---
          def read_file(p):
              try:
                  return p.read_text(encoding='utf-8', errors='ignore')
              except Exception:
                  return ""

          def parse_front_matter(text):
              if text.startswith('---'):
                  parts = text.split('---', 2)
                  if len(parts) >= 3:
                      try:
                          fm = yaml.safe_load(parts[1]) or {}
                          body = parts[2]
                          return fm, body
                      except Exception:
                          return {}, text
              return {}, text

          def extract_youtube_id(url):
              if not url:
                  return None
              try:
                  if "youtu.be/" in url:
                      return url.split("youtu.be/")[1].split("?")[0]
                  elif "youtube.com/watch" in url:
                      params = dict(x.split("=") for x in url.split("?")[1].split("&") if "=" in x)
                      return params.get("v")
              except Exception:
                  return None
              return None

          def extract_youtube_thumbnail(text, link=None):
              if link:
                  vid = extract_youtube_id(link)
                  if vid:
                      return f"https://img.youtube.com/vi/{vid}/hqdefault.jpg"
              urls = re.findall(r'(https?://(?:www\.)?(?:youtube\.com/watch\?v=|youtu\.be/)([A-Za-z0-9_-]{11}))', text)
              if urls:
                  video_id = urls[0][1]
                  return f"https://img.youtube.com/vi/{video_id}/hqdefault.jpg"
              return None

          # --- Build index.json ---
          posts = []

          if posts_root.exists():
              for p in sorted(posts_root.rglob('*')):
                  if p.is_file() and p.suffix.lower() in ('.md', '.markdown', '.html', '.htm'):
                      text = read_file(p)
                      meta = {}
                      body = text

                      # File modified time fallback
                      try:
                          mtime_ts = p.stat().st_mtime
                          mtime_dt = datetime.fromtimestamp(mtime_ts, tz=TZ_OFFSET)
                      except Exception:
                          mtime_dt = datetime.now(TZ_OFFSET)

                      # Parse front matter
                      if p.suffix.lower() in ('.md', '.markdown'):
                          fm, body = parse_front_matter(text)
                          if fm:
                              meta.update(fm or {})
                      else:
                          soup = BeautifulSoup(text, 'html.parser')
                          if soup.title and soup.title.string:
                              meta['title'] = soup.title.string.strip()
                          desc = soup.find('meta', attrs={'name':'description'}) or soup.find('meta', attrs={'property':'og:description'})
                          if desc and desc.get('content'):
                              meta['excerpt'] = desc['content'].strip()
                          kw = soup.find('meta', attrs={'name':'keywords'})
                          if kw and kw.get('content'):
                              meta.setdefault('tags', [t.strip() for t in kw['content'].split(',') if t.strip()])

                      if not meta.get('title'):
                          m = re.search(r'^\s*#\s+(.+)$', body, re.MULTILINE)
                          meta['title'] = m.group(1).strip() if m else p.stem

                      if not meta.get('excerpt'):
                          soup2 = BeautifulSoup(body, 'html.parser')
                          ptag = soup2.find('p')
                          meta['excerpt'] = ptag.get_text(strip=True)[:300] if ptag else ''

                      # Featured / Image
                      meta['featuredImage'] = None
                      meta['image'] = None
                      yt_thumb = extract_youtube_thumbnail(body, meta.get('link'))
                      if yt_thumb:
                          meta['featuredImage'] = yt_thumb
                          meta['image'] = yt_thumb
                      else:
                          soup2 = BeautifulSoup(body, 'html.parser')
                          img_tag = soup2.find('img')
                          if img_tag and img_tag.get('src'):
                              meta['featuredImage'] = img_tag['src'].strip()
                              meta['image'] = meta['featuredImage']
                          elif 'featuredImage' in meta and meta['featuredImage']:
                              meta['featuredImage'] = meta['featuredImage'].strip()
                              meta['image'] = meta['featuredImage']

                      tags = meta.get('tags') or meta.get('tag') or meta.get('keywords') or []
                      if isinstance(tags, str):
                          tags = [t.strip() for t in re.split(r'[,\\s]+', tags) if t.strip()]
                      categories = meta.get('categories') or meta.get('category') or []
                      if isinstance(categories, str):
                          categories = [t.strip() for t in re.split(r'[,\\s]+', categories) if t.strip()]

                      text_only = re.sub(r'<[^>]+>', '', body)
                      text_only = re.sub(r'[`*_>#~\-]{1,}', ' ', text_only)
                      words = len(re.findall(r'\w+', text_only))
                      reading_time = max(1, int(round(words / 200))) if words > 0 else 1

                      rel = p.relative_to(root).as_posix()
                      try:
                          parent_rel = p.parent.relative_to(root).as_posix()
                      except Exception:
                          parent_rel = ""
                      if p.name.lower() in ('index.html', 'index.htm'):
                          slug = '/' + parent_rel.rstrip('/') + '/'
                          if slug == '//':
                              slug = '/'
                      else:
                          slug = '/' + rel

                      dt_obj = None
                      if 'date' in meta and meta['date']:
                          try:
                              parsed = dateparser.parse(str(meta['date']))
                              if parsed:
                                  if not parsed.tzinfo:
                                      parsed = parsed.replace(tzinfo=timezone.utc)
                                  dt_obj = parsed.astimezone(TZ_OFFSET)
                          except Exception:
                              dt_obj = None

                      if not dt_obj:
                          dt_obj = mtime_dt

                      posts.append({
                          'title': meta.get('title'),
                          'date': dt_obj.isoformat(),
                          'excerpt': meta.get('excerpt', ''),
                          'path': slug,
                          'source': rel,
                          'tags': tags,
                          'categories': categories,
                          'reading_time': reading_time,
                          'word_count': words,
                          'featuredImage': meta.get('featuredImage'),
                          'image': meta.get('image')
                      })

          if not posts:
              posts = [{"title": "No posts yet", "excerpt": "", "path": "/", "source": ""}]

          posts_sorted = sorted(posts, key=lambda x: dateparser.parse(x['date']).timestamp(), reverse=True)
          index_path.write_text(json.dumps(posts_sorted, indent=2, ensure_ascii=False), encoding='utf-8')
          print(f"Written {index_path} with {len(posts_sorted)} posts.")

          # --- Build empty subscriptions.json if not exists ---
          if not subs_path.exists():
              subs_path.write_text(json.dumps({}, indent=2), encoding='utf-8')
              print(f"Created empty {subs_path}.")
          PY

      - name: Commit and push JSON files
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git pull --rebase --autostash origin main
          git add index.json subscriptions.json
          git commit -m "Update index.json and subscriptions.json [skip ci]" || echo "Nothing to commit"
          git push origin main
