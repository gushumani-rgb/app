name: Build index JSON

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  POSTS_DIR: Posts
  INDEX_PATH: index.json

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 pyyaml python-dateutil pytz

      - name: Build index.json
        run: |
          python - <<'PY'
          import os, json, re
          from pathlib import Path
          from bs4 import BeautifulSoup
          import yaml
          from dateutil import parser as dateparser
          from datetime import datetime, timezone, timedelta

          TZ_OFFSET = timezone(timedelta(hours=2))  # UTC+2
          root = Path.cwd()
          posts_root = root / os.environ.get("POSTS_DIR", "Posts")
          index_path = root / os.environ.get("INDEX_PATH", "index.json")

          def read_file(p):
              try:
                  return p.read_text(encoding='utf-8', errors='ignore')
              except Exception:
                  return ""

          def parse_front_matter(text):
              if text.startswith('---'):
                  parts = text.split('---', 2)
                  if len(parts) >= 3:
                      try:
                          fm = yaml.safe_load(parts[1]) or {}
                          body = parts[2]
                          return fm, body
                      except Exception:
                          return {}, text
              return {}, text

          posts = []

          if posts_root.exists():
              for p in sorted(posts_root.rglob('*')):
                  if p.is_file() and p.suffix.lower() in ('.md', '.markdown', '.html', '.htm'):
                      text = read_file(p)
                      meta = {}
                      body = text

                      try:
                          mtime_ts = p.stat().st_mtime
                          mtime_dt = datetime.fromtimestamp(mtime_ts, tz=TZ_OFFSET)
                      except Exception:
                          mtime_dt = datetime.now(TZ_OFFSET)

                      # Extract metadata
                      if p.suffix.lower() in ('.md', '.markdown'):
                          fm, body = parse_front_matter(text)
                          meta.update(fm or {})
                      else:
                          soup = BeautifulSoup(text, 'html.parser')
                          if soup.title and soup.title.string:
                              meta['title'] = soup.title.string.strip()
                          desc = soup.find('meta', attrs={'name':'description'}) or soup.find('meta', attrs={'property':'og:description'})
                          if desc and desc.get('content'):
                              meta['excerpt'] = desc['content'].strip()
                          kw = soup.find('meta', attrs={'name':'keywords'})
                          if kw and kw.get('content'):
                              meta.setdefault('tags', [t.strip() for t in kw['content'].split(',') if t.strip()])

                      if not meta.get('title'):
                          m = re.search(r'^\s*#\s+(.+)$', body, re.MULTILINE)
                          meta['title'] = m.group(1).strip() if m else p.stem

                      if not meta.get('excerpt'):
                          soup2 = BeautifulSoup(body, 'html.parser')
                          ptag = soup2.find('p')
                          meta['excerpt'] = ptag.get_text(strip=True)[:300] if ptag else ''

                      tags = meta.get('tags') or []
                      if isinstance(tags, str):
                          tags = [t.strip() for t in re.split(r'[,\s]+', tags) if t.strip()]
                      categories = meta.get('categories') or []
                      if isinstance(categories, str):
                          categories = [t.strip() for t in re.split(r'[,\s]+', categories) if t.strip()]

                      # Calculate reading time and word count
                      text_only = re.sub(r'<[^>]+>', '', body)
                      text_only = re.sub(r'[`*_>#~\-]{1,}', ' ', text_only)
                      words = len(re.findall(r'\w+', text_only))
                      reading_time = max(1, int(round(words / 200))) if words > 0 else 1

                      rel = p.relative_to(root).as_posix()
                      try:
                          parent_rel = p.parent.relative_to(root).as_posix()
                      except Exception:
                          parent_rel = ""
                      if p.name.lower() in ('index.html', 'index.htm'):
                          slug = '/' + parent_rel.rstrip('/') + '/'
                          if slug == '//':
                              slug = '/'
                      else:
                          slug = '/' + rel

                      # Parse date
                      dt_obj = None
                      if 'date' in meta and meta['date']:
                          try:
                              parsed = dateparser.parse(str(meta['date']))
                              if parsed:
                                  if not parsed.tzinfo:
                                      parsed = parsed.replace(tzinfo=timezone.utc)
                                  dt_obj = parsed.astimezone(TZ_OFFSET)
                          except Exception:
                              dt_obj = None

                      if not dt_obj:
                          soup_date = BeautifulSoup(body, 'html.parser')
                          pdate = soup_date.find('p', class_='post-date')
                          if pdate:
                              try:
                                  parsed = dateparser.parse(pdate.get_text(strip=True))
                                  if parsed:
                                      if not parsed.tzinfo:
                                          parsed = parsed.replace(tzinfo=timezone.utc)
                                      dt_obj = parsed.astimezone(TZ_OFFSET)
                              except Exception:
                                  dt_obj = None

                      if not dt_obj:
                          dt_obj = mtime_dt

                      posts.append({
                          'title': meta.get('title'),
                          'date': dt_obj.isoformat(),
                          'date_ts': dt_obj.timestamp(),
                          'excerpt': meta.get('excerpt', ''),
                          'path': slug,
                          'source': rel,
                          'tags': tags,
                          'categories': categories,
                          'reading_time': reading_time,
                          'word_count': words
                      })

          if not posts:
              posts = [{"title": "No posts yet", "excerpt": "", "path": "/", "source": ""}]

          posts_sorted = sorted(posts, key=lambda x: x.get('date_ts', 0), reverse=True)
          for p in posts_sorted:
              p.pop('date_ts', None)

          index_path.write_text(json.dumps(posts_sorted, indent=2, ensure_ascii=False), encoding='utf-8')
          print(f"Written {index_path} with {len(posts_sorted)} posts (UTC+2).")
          PY

      - name: Commit and push JSON
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add index.json
          git commit -m "Update index.json [skip ci]" || echo "Nothing to commit"
          git push origin main
